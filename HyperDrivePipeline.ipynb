{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import azureml.core\n",
        "import pandas as pd\n",
        "from azureml.core.runconfig import JarLibrary\n",
        "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
        "from azureml.exceptions import ComputeTargetException\n",
        "from azureml.core.datastore import Datastore\n",
        "from azureml.data.data_reference import DataReference\n",
        "from azureml.core.databricks import PyPiLibrary\n",
        "\n",
        "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
        "from azureml.core import Workspace, Environment, Experiment, Datastore, Dataset, ScriptRunConfig\n",
        "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\n",
        "from azureml.pipeline.steps import DatabricksStep, PythonScriptStep\n",
        "from azureml.train.hyperdrive import choice, loguniform\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SDK version: 1.47.0\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1674553599400
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "nlp-workspace\nopenaml\neastus2\nf9b97038-ed78-4a26-a1a7-51e81e75d867\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1674553600494
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.pipeline.core.pipeline_output_dataset import PipelineOutputAbstractDataset\n",
        "\n",
        "def_blob_store = ws.get_default_datastore()\n",
        "print('Datastore {} will be used'.format(def_blob_store.name))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Datastore workspaceblobstore will be used\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1674553600814
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_directory = \"./project\""
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1674553601246
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "cluster_name = \"gpu-cluster\"\n",
        "compute_target = ComputeTarget(workspace=ws, name=cluster_name)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1674553601749
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile conda_dependencies.yml\n",
        "# \n",
        "# channels:\n",
        "#   - pytorch\n",
        "#   - anaconda\n",
        "#   - conda-forge\n",
        "# dependencies:\n",
        "#   - python=3.7\n",
        "#   - pip=21.1.2\n",
        "#   - pip:\n",
        "#       - azureml-core==1.44.0\n",
        "#       - azureml-mlflow==1.44.0\n",
        "#       - azureml-automl-core==1.44.0\n",
        "#       - azureml-automl-dnn-nlp==1.44.0\n",
        "#       - azureml-responsibleai==1.44.0\n",
        "#       - azureml-automl-runtime==1.44.0\n",
        "#       - azureml-train-automl-client==1.44.0\n",
        "#       - azureml-train-automl-runtime==1.44.0\n",
        "#       - horovod==0.21.3\n",
        "#   - numpy~=1.18.5\n",
        "#   - pandas~=1.1.5\n",
        "#   - scikit-learn~=0.22.1\n",
        "#   - pytorch==1.7.1"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1674553602067
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'nlp-accelerator' not in ws.environments:\n",
        "    base_env = Environment.get(workspace=ws, name=\"AzureML-AutoML-DNN-Text-GPU\")\n",
        "    env = base_env.clone(\"nlp-accelerator\")\n",
        "\n",
        "    conda_dep = env.python.conda_dependencies\n",
        "    conda_dep.add_pip_package('nvitop')\n",
        "\n",
        "    env.python.conda_dependencies = conda_dep\n",
        "\n",
        "    env.register(ws)\n",
        "else:\n",
        "    env = Environment.get(workspace=ws, name=\"nlp-accelerator\")\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1674553602393
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to get larger datasets: http://jmcauley.ucsd.edu/data/amazon/"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1674553602706
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Automotive_5.json.gz -P data/"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "--2023-01-24 09:46:43--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Automotive_5.json.gz\r\nResolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\r\nConnecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 4669048 (4.5M) [application/x-gzip]\r\nSaving to: ‘data/reviews_Automotive_5.json.gz.88’\r\n\r\n\r          reviews_A   0%[                    ]       0  --.-KB/s               \r         reviews_Au   2%[                    ] 103.97K   391KB/s               \r        reviews_Aut   5%[>                   ] 249.00K   518KB/s               "
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "\n",
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    yield eval(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "pdf_main = getDF('data/reviews_Automotive_5.json.gz')\n",
        "pdf_main.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086238720
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_main.loc[pdf_main['overall'] >= 4, 'sentiment'] = 1\n",
        "pdf_main.loc[pdf_main['overall'] < 3, 'sentiment'] = 0\n",
        "\n",
        "pdf_main.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086238929
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_datasets(pdf_target_training, label = 'sentiment'):\n",
        "    X_train, X_test_val, y_train, y_test_val = train_test_split(pdf_target_training.drop(label, axis=1), pdf_target_training[label],\n",
        "                                                        stratify=pdf_target_training[label],\n",
        "                                                        shuffle=True,\n",
        "                                                        test_size=0.20)\n",
        "\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val,\n",
        "                                                        stratify=y_test_val,\n",
        "                                                        shuffle=True,\n",
        "                                                        test_size=0.5)\n",
        "    pdf_X_train = X_train\n",
        "    pdf_X_val = X_val\n",
        "    pdf_X_test = X_test\n",
        "\n",
        "    pdf_X_train['sentiment'] = y_train\n",
        "    pdf_X_val['sentiment'] = y_val\n",
        "    pdf_X_test['sentiment'] = y_test\n",
        "    \n",
        "    print(f'Total records for: \"pdf_X_train\": [{pdf_X_train.shape[0]}]')\n",
        "    print(f'Total records for: \"pdf_X_val\": [{pdf_X_val.shape[0]}]')\n",
        "    print(f'Total records for: \"pdf_X_test\": [{pdf_X_test.shape[0]}]')\n",
        "    \n",
        "    return pdf_X_train, pdf_X_val, pdf_X_test"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086239108
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_train, pdf_val, pdf_test = generate_datasets(pdf_main[['reviewText', 'sentiment']].dropna(), 'sentiment')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086239386
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def_blob_store = ws.get_default_datastore()\n",
        "\n",
        "ds_train_set = Dataset.Tabular.register_pandas_dataframe(dataframe=pdf_train, target=(def_blob_store, 'nlp'), name=\"train_set\", description=\"Small amazon review for sentiment analysis [train set]\")\n",
        "ds_val_set = Dataset.Tabular.register_pandas_dataframe(dataframe=pdf_val, target=(def_blob_store, 'nlp'), name=\"val_set\", description=\"Small amazon review for sentiment analysis [val set]\")\n",
        "ds_test_set = Dataset.Tabular.register_pandas_dataframe(dataframe=pdf_test, target=(def_blob_store, 'nlp'), name=\"test_set\", description=\"Small amazon review for sentiment analysis [test set]\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086240878
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig\n",
        "\n",
        "args = [\n",
        "        '--target-name', 'sentiment',\n",
        "        '--training-dataset', ds_train_set.as_named_input('train_set'),\n",
        "        '--val-dataset', ds_val_set.as_named_input('val_set'),\n",
        "        '--test-dataset', ds_test_set.as_named_input('test_set'),\n",
        "        '--text-field', 'reviewText',\n",
        "        '--is-test', 1,\n",
        "        '--is-final', 0,\n",
        "        '--is-jump', 0,\n",
        "        '--is-local', 0,\n",
        "        '--evaluation-strategy', \"epoch\",\n",
        "        '--collect-resource-utilization', 1, # \n",
        "        '--resource-utilization-interval', 5.0 # seconds\n",
        "]\n",
        "\n",
        "src = ScriptRunConfig(source_directory=source_directory,\n",
        "                      script='train_transformer.py',\n",
        "                      arguments=args,\n",
        "                      compute_target=compute_target,\n",
        "                      environment=env)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086241039
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
        "from azureml.train.hyperdrive import choice, loguniform\n",
        "\n",
        "ps = RandomParameterSampling(\n",
        "    {\n",
        "        # bert-base-cased model could fit into all NC series, but if you're interested in trying larger models, then you need to make sure the VM type can handle the size of the model\n",
        "        '--base-checkpoint': choice(\"bert-base-cased\"), #, \"bert-base-cased\"), # , \"bert-large-cased\", \"microsoft/deberta-v3-small\", \"distilbert-base-uncased\", \"bert-base-uncased\"),\n",
        "        '--batch-size': choice(8),\n",
        "        '--no-epochs': choice(4),\n",
        "        '--learning-rate': choice(5.5e-5, 5e-5, 4.5e-5, 4e-5, 5.5e-5, 6e-5, 3.5e-5, 6.5e-5),\n",
        "        '--warmup-steps': choice(0),\n",
        "        '--weight-decay': choice(0.0),\n",
        "        '--adam-beta1': choice(0.9),\n",
        "        '--adam-beta2': choice(0.999),\n",
        "        '--adam-epsilon': choice(1e-8)\n",
        "    }\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086241211
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = BanditPolicy(evaluation_interval=5, slack_factor=0.1)\n",
        "hyperdrive_config = HyperDriveConfig(run_config=src,\n",
        "                                     hyperparameter_sampling=ps,\n",
        "                                     policy=policy,\n",
        "                                     primary_metric_name='test_AUC_weighted',\n",
        "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
        "                                     max_total_runs=20,\n",
        "                                     max_concurrent_runs=3)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086241397
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep\n",
        "\n",
        "hd_step_name='HyperDrive_Step'\n",
        "hd_step = HyperDriveStep(\n",
        "    name=hd_step_name,\n",
        "    hyperdrive_config=hyperdrive_config,\n",
        "    allow_reuse=True)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086241572
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "### Test AutoML Trained Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# choose a name for your cluster\n",
        "cpu_compute = ComputeTarget(workspace=ws, name=\"cpu-cluster\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086242358
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_cpu = Environment.get(workspace=ws, name=\"AzureML-AutoML-DNN\") #name=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086242632
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.runconfig import RunConfiguration\n",
        "\n",
        "rcfg = RunConfiguration()\n",
        "rcfg.environment = env_cpu\n",
        "\n",
        "register_model_step = PythonScriptStep(script_name='register_model.py',\n",
        "                                       source_directory=source_directory,\n",
        "                                       name=\"Register_Best_Model\",\n",
        "                                       compute_target=cpu_compute,\n",
        "                                       arguments=['--is-test', 0,\n",
        "                                                  '--test-run-id', '',\n",
        "                                                  '--metric-name', 'test_AUC_weighted',\n",
        "                                                  '--target-name', 'sentiment',\n",
        "                                                  '--model-name', 'sentiment_classifier',\n",
        "                                                \n",
        "                                                  ],\n",
        "                                             \n",
        "                                       allow_reuse=True,\n",
        "                                       runconfig=rcfg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "register_model_step.run_after(hd_step)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086242808
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rcfg = RunConfiguration()\n",
        "rcfg.environment = env_cpu\n",
        "\n",
        "deploy_model_step = PythonScriptStep(script_name='deploy_model.py',\n",
        "                                       source_directory=source_directory,\n",
        "                                       name=\"Deploy_Latest_Model\",\n",
        "                                       compute_target=cpu_compute,\n",
        "                                       arguments=['--endpoint-name', 'sentiment-endpoint-2',\n",
        "                                                  '--model-name', 'sentiment_classifier'],\n",
        "                                       allow_reuse=True,\n",
        "                                       runconfig=rcfg)\n",
        "\n",
        "deploy_model_step.run_after(register_model_step)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086242976
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp = Experiment(workspace=ws, name='transformer_hp')\n",
        "steps = [deploy_model_step]\n",
        "pipeline = Pipeline(workspace=ws, steps=steps)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086243228
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.submit(exp.name) #, credential_passthrough=True)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086254277
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "timenow = datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
        "\n",
        "pipeline_name = f\"Sentiment-Classifier-{timenow}-Pipeline\"\n",
        "print(pipeline_name)\n",
        "\n",
        "published_pipeline = pipeline.publish(\n",
        "    name=pipeline_name, \n",
        "    description=pipeline_name)\n",
        "print(\"Newly published pipeline id: {}\".format(published_pipeline.id))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674086254839
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}