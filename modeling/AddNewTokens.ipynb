{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import glob\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn import preprocessing\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from datasets import Dataset, DatasetDict\n",
        "import plotly.graph_objects as go\n",
        "from IPython.core.display import HTML\n",
        "from utils import *\n",
        "\n",
        "from azureml.core import Workspace, Environment, Experiment, Datastore, Dataset, ScriptRunConfig\n",
        "from azureml.train.automl.run import AutoMLRun\n",
        "from azureml.core.datastore import Datastore\n",
        "from azureml.data.data_reference import DataReference\n",
        "\n",
        "from captum.attr import visualization as viz\n",
        "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
        "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check core SDK version number\n",
        "# print(\"SDK version:\", azureml.core.VERSION)\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1659396724778
        }
      },
      "id": "38f12750"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\r\n",
        "\r\n",
        "sys.path.append(os.getcwd() + \"/project\")\r\n",
        "\r\n",
        "from train_transformer import get_model, adjust_tokenizer, compute_metrics, get_encode_labels, tokenize_function, generate_tokenized_dataset, get_datasets"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659396725096
        }
      },
      "id": "f82f6f8c-1a9a-4684-a07a-794adf888dec"
    },
    {
      "cell_type": "code",
      "source": [
        "base_checkpoint = \"bert-base-uncased\"\r\n",
        "text_field_name = \"TEXT_FINAL\"\r\n",
        "target_name = \"target\"\r\n",
        "batch_size = 16\r\n",
        "is_test = 1\r\n",
        "is_local = 1\r\n",
        "no_epochs = 3\r\n",
        "\r\n",
        "if is_test:\r\n",
        "    no_epochs = 2"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659392658612
        }
      },
      "id": "3bb9e4a9-a2b1-4cca-b981-cc4fca346407"
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(os.getcwd() + \"/project\")\r\n",
        "os.getcwd()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/hossein-nc6-ci/code/Users/hosarsha/og_classification/project'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659392659489
        }
      },
      "id": "cc220d25-a73d-45be-abc1-bfdbd61271a2"
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_train, pdf_validation, pdf_test, pdf_temporal_test = get_datasets(is_local, is_test)\r\n",
        "\r\n",
        "num_labels = len(pdf_train[target_name].unique())\r\n",
        "print(f'num_labels: {num_labels}')\r\n",
        "\r\n",
        "model, tokenizer = get_model(base_checkpoint, num_labels)\r\n",
        "model, tokenizer = adjust_tokenizer(model, tokenizer)\r\n",
        "\r\n",
        "le = get_encode_labels(pdf_train, target_name)\r\n",
        "\r\n",
        "fields = [text_field_name, target_name, 'labels']\r\n",
        "\r\n",
        "train_ds, tokenized_train_ds = generate_tokenized_dataset(pdf_train, fields, le, target_name, text_field_name, tokenizer)\r\n",
        "validation_ds, tokenized_validation_ds = generate_tokenized_dataset(pdf_validation, fields, le, target_name, text_field_name, tokenizer)\r\n",
        "test_ds, tokenized_test_ds = generate_tokenized_dataset(pdf_test, fields, le, target_name, text_field_name, tokenizer)\r\n",
        "temporal_test_ds, tokenized_temporal_test_ds = generate_tokenized_dataset(pdf_temporal_test, fields, le, target_name, text_field_name, tokenizer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "the job is running locally\nthe job is a test job\npdf_train is imported wit \"(2500, 2)\" rows\npdf_validation is imported wit \"(2500, 2)\" rows\npdf_test is imported wit \"(2500, 2)\" rows\npdf_temporal_test is imported wit \"(2500, 2)\" rows\nnum_labels: 51\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/3 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf82bea50743468194566e8ee99f8f52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/3 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e21de9ac36304e8da5659b40f7f3af5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/3 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e66f564ea84f45bd8991e7859d3b3344"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/3 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94d78f3b1d9c40d5b740a3a59630a78e"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659392673183
        }
      },
      "id": "f0794c30-5caa-4eb3-ad98-f15c6682cd60"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenized data is generated')\r\n",
        "\r\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Tokenized data is generated\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659392673497
        }
      },
      "id": "b72e627f-5435-4e6d-ab88-b582bd108555"
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\r\n",
        "    output_dir=\"outputs\",\r\n",
        "    evaluation_strategy=\"epoch\",\r\n",
        "    eval_steps=500,\r\n",
        "    per_device_train_batch_size=batch_size,\r\n",
        "    per_device_eval_batch_size=batch_size,\r\n",
        "    num_train_epochs=no_epochs,\r\n",
        "    seed=0,\r\n",
        "    load_best_model_at_end=True,\r\n",
        ")\r\n",
        "\r\n",
        "trainer = Trainer(\r\n",
        "    model=model,\r\n",
        "    args=args,\r\n",
        "    train_dataset=tokenized_train_ds,\r\n",
        "    eval_dataset=tokenized_validation_ds,\r\n",
        "    data_collator=data_collator,\r\n",
        "    tokenizer=tokenizer,\r\n",
        "    compute_metrics=compute_metrics,\r\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659392690352
        }
      },
      "id": "7f10f4fa-5bff-45b3-bf94-4a7fe4efee17"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train pre-trained model\r\n",
        "print(\"Training started\")\r\n",
        "trainer.train()\r\n",
        "print(\"Training is finished\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training started\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Trainer is attempting to log a value of \"{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7', 8: 'LABEL_8', 9: 'LABEL_9', 10: 'LABEL_10', 11: 'LABEL_11', 12: 'LABEL_12', 13: 'LABEL_13', 14: 'LABEL_14', 15: 'LABEL_15', 16: 'LABEL_16', 17: 'LABEL_17', 18: 'LABEL_18', 19: 'LABEL_19', 20: 'LABEL_20', 21: 'LABEL_21', 22: 'LABEL_22', 23: 'LABEL_23', 24: 'LABEL_24', 25: 'LABEL_25', 26: 'LABEL_26', 27: 'LABEL_27', 28: 'LABEL_28', 29: 'LABEL_29', 30: 'LABEL_30', 31: 'LABEL_31', 32: 'LABEL_32', 33: 'LABEL_33', 34: 'LABEL_34', 35: 'LABEL_35', 36: 'LABEL_36', 37: 'LABEL_37', 38: 'LABEL_38', 39: 'LABEL_39', 40: 'LABEL_40', 41: 'LABEL_41', 42: 'LABEL_42', 43: 'LABEL_43', 44: 'LABEL_44', 45: 'LABEL_45', 46: 'LABEL_46', 47: 'LABEL_47', 48: 'LABEL_48', 49: 'LABEL_49', 50: 'LABEL_50'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\nTrainer is attempting to log a value of \"{'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6, 'LABEL_7': 7, 'LABEL_8': 8, 'LABEL_9': 9, 'LABEL_10': 10, 'LABEL_11': 11, 'LABEL_12': 12, 'LABEL_13': 13, 'LABEL_14': 14, 'LABEL_15': 15, 'LABEL_16': 16, 'LABEL_17': 17, 'LABEL_18': 18, 'LABEL_19': 19, 'LABEL_20': 20, 'LABEL_21': 21, 'LABEL_22': 22, 'LABEL_23': 23, 'LABEL_24': 24, 'LABEL_25': 25, 'LABEL_26': 26, 'LABEL_27': 27, 'LABEL_28': 28, 'LABEL_29': 29, 'LABEL_30': 30, 'LABEL_31': 31, 'LABEL_32': 32, 'LABEL_33': 33, 'LABEL_34': 34, 'LABEL_35': 35, 'LABEL_36': 36, 'LABEL_37': 37, 'LABEL_38': 38, 'LABEL_39': 39, 'LABEL_40': 40, 'LABEL_41': 41, 'LABEL_42': 42, 'LABEL_43': 43, 'LABEL_44': 44, 'LABEL_45': 45, 'LABEL_46': 46, 'LABEL_47': 47, 'LABEL_48': 48, 'LABEL_49': 49, 'LABEL_50': 50}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [312/312 04:23, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Recall Weighted</th>\n      <th>Precision Weighted</th>\n      <th>F1 Weighted</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>2.199767</td>\n      <td>0.534455</td>\n      <td>0.033343</td>\n      <td>0.053389</td>\n      <td>0.038959</td>\n      <td>0.534455</td>\n      <td>0.318269</td>\n      <td>0.396530</td>\n      <td>21.257300</td>\n      <td>117.418000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>2.047934</td>\n      <td>0.568910</td>\n      <td>0.110769</td>\n      <td>0.099510</td>\n      <td>0.089693</td>\n      <td>0.568910</td>\n      <td>0.393976</td>\n      <td>0.449391</td>\n      <td>21.321000</td>\n      <td>117.068000</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Attempted to log scalar metric eval_loss:\n2.1997668743133545\nAttempted to log scalar metric eval_accuracy:\n0.5344551282051282\nAttempted to log scalar metric eval_precision:\n0.033342896199472206\nAttempted to log scalar metric eval_recall:\n0.05338930421811698\nAttempted to log scalar metric eval_f1:\n0.038959419892849796\nAttempted to log scalar metric eval_recall_weighted:\n0.5344551282051282\nAttempted to log scalar metric eval_precision_weighted:\n0.3182694635194428\nAttempted to log scalar metric eval_f1_weighted:\n0.3965299591993869\nAttempted to log scalar metric eval_runtime:\n21.2573\nAttempted to log scalar metric eval_samples_per_second:\n117.418\nAttempted to log scalar metric epoch:\n1.0\nAttempted to log scalar metric eval_loss:\n2.04793381690979\nAttempted to log scalar metric eval_accuracy:\n0.5689102564102564\nAttempted to log scalar metric eval_precision:\n0.11076915248920266\nAttempted to log scalar metric eval_recall:\n0.09950983119111605\nAttempted to log scalar metric eval_f1:\n0.08969327532832054\nAttempted to log scalar metric eval_recall_weighted:\n0.5689102564102564\nAttempted to log scalar metric eval_precision_weighted:\n0.39397566183020016\nAttempted to log scalar metric eval_f1_weighted:\n0.4493911422765619\nAttempted to log scalar metric eval_runtime:\n21.321\nAttempted to log scalar metric eval_samples_per_second:\n117.068\nAttempted to log scalar metric epoch:\n2.0\nAttempted to log scalar metric train_runtime:\n266.1025\nAttempted to log scalar metric train_samples_per_second:\n1.172\nAttempted to log scalar metric total_flos:\n1361332266106176.0\nAttempted to log scalar metric epoch:\n2.0\nTraining is finished\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659392963965
        }
      },
      "id": "72d68f0c-a0f1-4924-bd39-5d5b4bae5c61"
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(trainer, ds, prefix):\r\n",
        "    test_result = trainer.predict(tokenized_test_ds)\r\n",
        "\r\n",
        "    metrics = test_result.metrics.keys()\r\n",
        "    # print(f'len(metrics): {metrics}')\r\n",
        "    \r\n",
        "    for m in metrics:\r\n",
        "        print(f'{prefix}_{m.replace(\"test_\", \"\")}', f'{test_result.metrics[m]}')\r\n"
      ],
      "outputs": [],
      "execution_count": 55,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659396520881
        }
      },
      "id": "d7915b66-636f-4eec-a57b-e1f1376f3bc5"
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(trainer, tokenized_test_ds, 'test')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "test_loss 1.9834322929382324\ntest_accuracy 0.5886354541816726\ntest_precision 0.10803025391935897\ntest_recall 0.09252521854691055\ntest_f1 0.08474615273562676\ntest_recall_weighted 0.5886354541816726\ntest_precision_weighted 0.4158531557489751\ntest_f1_weighted 0.4722916259768107\ntest_runtime 21.2984\ntest_samples_per_second 117.333\ntest_mem_cpu_alloc_delta 0\ntest_mem_gpu_alloc_delta 0\ntest_mem_cpu_peaked_delta 0\ntest_mem_gpu_peaked_delta 579536896\n"
        }
      ],
      "execution_count": 56,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659396543969
        }
      },
      "id": "222e140a-4c69-431f-9263-a8f2fce564d4"
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(trainer, tokenized_temporal_test_ds, 'temporal_test')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "temporal_test_loss 1.9834322929382324\ntemporal_test_accuracy 0.5886354541816726\ntemporal_test_precision 0.10803025391935897\ntemporal_test_recall 0.09252521854691055\ntemporal_test_f1 0.08474615273562676\ntemporal_test_recall_weighted 0.5886354541816726\ntemporal_test_precision_weighted 0.4158531557489751\ntemporal_test_f1_weighted 0.4722916259768107\ntemporal_test_runtime 21.4312\ntemporal_test_samples_per_second 116.606\ntemporal_test_mem_cpu_alloc_delta 0\ntemporal_test_mem_gpu_alloc_delta 0\ntemporal_test_mem_cpu_peaked_delta 0\ntemporal_test_mem_gpu_peaked_delta 579536896\n"
        }
      ],
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659396565884
        }
      },
      "id": "e142bf4e-4709-4ef4-8726-b956f01856a1"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659392640955
        }
      },
      "id": "abc536f9-5bee-4c76-97f7-8d065445d6da"
    },
    {
      "cell_type": "code",
      "source": [
        "a = trainer.predict(tokenized_test_ds)\r\n",
        "\r\n",
        "# tokenized_temporal_test_ds"
      ],
      "outputs": [],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659396017806
        }
      },
      "id": "6d4bee12-eca9-40f8-ba31-bb736cc68eeb"
    },
    {
      "cell_type": "code",
      "source": [
        "type(a)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "transformers.trainer_utils.PredictionOutput"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659392986396
        }
      },
      "id": "c58e4c69-ec6b-4b11-a4b7-63669a3802a1"
    },
    {
      "cell_type": "code",
      "source": [
        "a.metrics"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 43,
          "data": {
            "text/plain": "{'test_loss': 1.9834322929382324,\n 'test_accuracy': 0.5886354541816726,\n 'test_precision': 0.10803025391935897,\n 'test_recall': 0.09252521854691055,\n 'test_f1': 0.08474615273562676,\n 'test_recall_weighted': 0.5886354541816726,\n 'test_precision_weighted': 0.4158531557489751,\n 'test_f1_weighted': 0.4722916259768107,\n 'test_runtime': 21.3746,\n 'test_samples_per_second': 116.914,\n 'test_mem_cpu_alloc_delta': 0,\n 'test_mem_gpu_alloc_delta': 0,\n 'test_mem_cpu_peaked_delta': 0,\n 'test_mem_gpu_peaked_delta': 579536896}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1659396023198
        }
      },
      "id": "2699b4a8-685f-4c4f-9b51-46dfa177af74"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"outputs/model\")\r\n",
        "\r\n",
        "# to save encoder \r\n",
        "joblib.dump(le,'outputs/model/labelEncoder.joblib',compress=9)\r\n",
        "print(\"Model and encoder are saved\")\r\n",
        "\r\n",
        "print(\"Evaluation is started\")\r\n",
        "trainer.evaluate()\r\n",
        "print(\"Evaluation is completed\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "491a32f9-e556-4034-b775-2a8c42ef8017"
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1659375248599
        }
      },
      "id": "648b557d"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}