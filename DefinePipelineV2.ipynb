{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1668533978662
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dac8073e-1c2d-4a7d-a53b-c3655e291d58\n",
            "Learning\n",
            "learningmain\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Enter details of your AML workspace\n",
        "subscription_id = \"dac8073e-1c2d-4a7d-a53b-c3655e291d58\"\n",
        "resource_group = \"Learning\"\n",
        "workspace = \"learningmain\"\n",
        "\n",
        "# get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
        ")\n",
        "\n",
        "print(ml_client.subscription_id, ml_client.resource_group_name, ml_client.workspace_name, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-01-25 17:10:03--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Automotive_5.json.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4669048 (4.5M) [application/x-gzip]\n",
            "Saving to: ‘data/reviews_Automotive_5.json.gz.20’\n",
            "\n",
            "reviews_Automotive_ 100%[===================>]   4.45M  2.54MB/s    in 1.8s    \n",
            "\n",
            "2023-01-25 17:10:05 (2.54 MB/s) - ‘data/reviews_Automotive_5.json.gz.20’ saved [4669048/4669048]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# to get larger datasets, visit: http://jmcauley.ucsd.edu/data/amazon/\n",
        "\n",
        "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Automotive_5.json.gz -P data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20473, 9)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "\n",
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    yield eval(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "pdf_main = getDF('data/reviews_Automotive_5.json.gz')\n",
        "pdf_main.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>20473.000000</td>\n",
              "      <td>2.047300e+04</td>\n",
              "      <td>19043.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.471841</td>\n",
              "      <td>1.365018e+09</td>\n",
              "      <td>0.939715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.940337</td>\n",
              "      <td>3.621266e+07</td>\n",
              "      <td>0.238020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.121386e+09</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.354838e+09</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.373414e+09</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.390435e+09</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.405901e+09</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            overall  unixReviewTime     sentiment\n",
              "count  20473.000000    2.047300e+04  19043.000000\n",
              "mean       4.471841    1.365018e+09      0.939715\n",
              "std        0.940337    3.621266e+07      0.238020\n",
              "min        1.000000    1.121386e+09      0.000000\n",
              "25%        4.000000    1.354838e+09      1.000000\n",
              "50%        5.000000    1.373414e+09      1.000000\n",
              "75%        5.000000    1.390435e+09      1.000000\n",
              "max        5.000000    1.405901e+09      1.000000"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf_main.loc[pdf_main['overall'] >= 4, 'sentiment'] = 1\n",
        "pdf_main.loc[pdf_main['overall'] < 3, 'sentiment'] = 0\n",
        "\n",
        "pdf_main.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def generate_datasets(pdf_target_training, label = 'sentiment'):\n",
        "    X_train, X_test_val, y_train, y_test_val = train_test_split(pdf_target_training.drop(label, axis=1), pdf_target_training[label],\n",
        "                                                        stratify=pdf_target_training[label],\n",
        "                                                        shuffle=True,\n",
        "                                                        test_size=0.20)\n",
        "\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val,\n",
        "                                                        stratify=y_test_val,\n",
        "                                                        shuffle=True,\n",
        "                                                        test_size=0.5)\n",
        "    pdf_X_train = X_train\n",
        "    pdf_X_val = X_val\n",
        "    pdf_X_test = X_test\n",
        "\n",
        "    pdf_X_train['sentiment'] = y_train\n",
        "    pdf_X_val['sentiment'] = y_val\n",
        "    pdf_X_test['sentiment'] = y_test\n",
        "    \n",
        "    print(f'Total records for: \"pdf_X_train\": [{pdf_X_train.shape[0]}]')\n",
        "    print(f'Total records for: \"pdf_X_val\": [{pdf_X_val.shape[0]}]')\n",
        "    print(f'Total records for: \"pdf_X_test\": [{pdf_X_test.shape[0]}]')\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    return pdf_X_train, pdf_X_val, pdf_X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records for: \"pdf_X_train\": [15234]\n",
            "Total records for: \"pdf_X_val\": [1904]\n",
            "Total records for: \"pdf_X_test\": [1905]\n"
          ]
        }
      ],
      "source": [
        "pdf_train, pdf_val, pdf_test = generate_datasets(pdf_main[['reviewText', 'sentiment']].dropna(), 'sentiment')\n",
        "\n",
        "pdf_train.to_csv('data/pdf_train.csv')\n",
        "pdf_val.to_csv('data/pdf_val.csv')\n",
        "pdf_test.to_csv('data/pdf_test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "# === Note on path ===\n",
        "# can be can be a local path or a cloud path. AzureML supports https://`, `abfss://`, `wasbs://` and `azureml://` URIs.\n",
        "# Local paths are automatically uploaded to the default datastore in the cloud.\n",
        "# More details on supported paths: https://docs.microsoft.com/azure/machine-learning/how-to-read-write-data-v2#supported-paths\n",
        "\n",
        "def gen_input_data(url):\n",
        "    return Input(type=AssetTypes.URI_FILE, path=url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_train = gen_input_data('data/pdf_train.csv')\n",
        "ds_val = gen_input_data('data/pdf_val.csv')\n",
        "ds_test = gen_input_data('data/pdf_test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1668533982929
        }
      },
      "outputs": [],
      "source": [
        "source_directory = \"./project_v2/\"\n",
        "experiment_name = 'transformer_hp_v2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1668533983332
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting environments/conda_dependencies.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile environments/conda_dependencies.yml\n",
        "\n",
        "channels:\n",
        "  - pytorch\n",
        "  - anaconda\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.7\n",
        "  - pip=21.1.2\n",
        "  - pip:\n",
        "      - azure-ai-ml==1.2.0\n",
        "      - mlflow== 1.26.1\n",
        "      - azureml-mlflow==1.42.0\n",
        "      - nvitop\n",
        "      - transformers\n",
        "      - inference-schema\n",
        "      - joblib\n",
        "      - datasets\n",
        "      - sentencepiece\n",
        "  - numpy~=1.21.6\n",
        "  - pandas~=1.1.5\n",
        "  - shap=0.39.0\n",
        "  - scikit-learn~=0.22.1\n",
        "  - pytorch==1.7.1\n",
        "name: nlp_training_environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Environment({'is_anonymous': False, 'auto_increment_version': False, 'name': 'nlp-accelerator-sdk-v2', 'description': 'This environment is curated to run NLP Transformer based models using AML SDK-v2 and native MLFlow integration', 'tags': {}, 'properties': {}, 'id': '/subscriptions/dac8073e-1c2d-4a7d-a53b-c3655e291d58/resourceGroups/Learning/providers/Microsoft.MachineLearningServices/workspaces/learningmain/environments/nlp-accelerator-sdk-v2/versions/12', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/hosarsha23/code/Users/hosarsha/nlp-aml', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7ff5cebcad40>, 'serialize': <msrest.serialization.Serializer object at 0x7ff5cebcb040>, 'version': '12', 'latest_version': None, 'conda_file': {'channels': ['pytorch', 'anaconda', 'conda-forge'], 'dependencies': ['python=3.7', 'pip=21.1.2', {'pip': ['azure-ai-ml==1.2.0', 'mlflow== 1.26.1', 'azureml-mlflow==1.42.0', 'nvitop', 'transformers', 'inference-schema', 'joblib', 'datasets', 'sentencepiece']}, 'numpy~=1.21.6', 'pandas~=1.1.5', 'shap=0.39.0', 'scikit-learn~=0.22.1', 'pytorch==1.7.1'], 'name': 'nlp_training_environment'}, 'image': 'mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu20.04:latest', 'build': None, 'inference_config': None, 'os_type': 'Linux', 'arm_type': 'environment_version', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': '{\\n  \"channels\": [\\n    \"pytorch\",\\n    \"anaconda\",\\n    \"conda-forge\"\\n  ],\\n  \"dependencies\": [\\n    \"python=3.7\",\\n    \"pip=21.1.2\",\\n    {\\n      \"pip\": [\\n        \"azure-ai-ml==1.2.0\",\\n        \"mlflow== 1.26.1\",\\n        \"azureml-mlflow==1.42.0\",\\n        \"nvitop\",\\n        \"transformers\",\\n        \"inference-schema\",\\n        \"joblib\",\\n        \"datasets\",\\n        \"sentencepiece\"\\n      ]\\n    },\\n    \"numpy~=1.21.6\",\\n    \"pandas~=1.1.5\",\\n    \"shap=0.39.0\",\\n    \"scikit-learn~=0.22.1\",\\n    \"pytorch==1.7.1\"\\n  ],\\n  \"name\": \"nlp_training_environment\"\\n}'})"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "env_name = 'nlp-accelerator-sdk-v2'\n",
        "env_list = list(ml_client.environments.list(name=env_name))\n",
        "if len(env_list) > 0:\n",
        "    env = env_list[0]\n",
        "else:\n",
        "    env = Environment(\n",
        "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu20.04:latest\",\n",
        "        conda_file='environments/conda_dependencies.yml',\n",
        "        name=env_name,\n",
        "        description='This environment is curated to run NLP Transformer based models using AML SDK-v2 and native MLFlow integration'\n",
        "    )\n",
        "\n",
        "    ml_client.environments.create_or_update(env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "gather": {
          "logged": 1668533987254
        }
      },
      "outputs": [],
      "source": [
        "cluster_name = \"a100-cluster\"\n",
        "compute_target = ml_client.compute.get(cluster_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "cpu_compute_target = ml_client.compute.get(\"cpu-cluster\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "job_train = command(\n",
        "    inputs=dict(\n",
        "        training_dataset=ds_train,\n",
        "        val_dataset=ds_val,\n",
        "        test_dataset=ds_test,\n",
        "        target_name='sentiment', \n",
        "        text_field_name='reviewText',\n",
        "        is_test=0,\n",
        "        is_final=0,\n",
        "        is_local=0,\n",
        "        is_jump=0,\n",
        "        evaluation_strategy='epoch',\n",
        "        collect_resource_utilization=1,\n",
        "        resource_utilization_interval=5.0, # seconds\n",
        "        base_checkpoint='bert-base-cased',\n",
        "        batch_size=8,\n",
        "        no_epochs=4,\n",
        "        learning_rate=5.5e-5,\n",
        "        warmup_steps=0,\n",
        "        weight_decay=0.0,\n",
        "        adam_beta1=0.9,\n",
        "        adam_beta2=0.999,\n",
        "        adam_epsilon=1e-8\n",
        "    ),\n",
        "    outputs=dict(\n",
        "        model_output=Output(type=\"custom_model\")\n",
        "    ),\n",
        "    compute=compute_target,\n",
        "    environment=env,\n",
        "    code=source_directory, # location of source code\n",
        "    command=\"\"\"\n",
        "    python train_transformer.py \\\n",
        "        --collect-resource-utilization ${{inputs.collect_resource_utilization}} \\\n",
        "        --resource-utilization-interval ${{inputs.resource_utilization_interval}} \\\n",
        "        --target-name ${{inputs.target_name}} \\\n",
        "        --training-dataset ${{inputs.training_dataset}} \\\n",
        "        --val-dataset ${{inputs.val_dataset}} \\\n",
        "        --test-dataset ${{inputs.test_dataset}} \\\n",
        "        --model-path ${{outputs.model_output}} \\\n",
        "        --text-field-name ${{inputs.text_field_name}} \\\n",
        "        --is-test ${{inputs.is_test}} \\\n",
        "        --is-final ${{inputs.is_final}} \\\n",
        "        --is-local ${{inputs.is_local}} \\\n",
        "        --is-jump ${{inputs.is_jump}} \\\n",
        "        --evaluation-strategy ${{inputs.evaluation_strategy}} \\\n",
        "        --base-checkpoint ${{inputs.base_checkpoint}} \\\n",
        "        --batch-size ${{inputs.batch_size}} \\\n",
        "        --no-epochs ${{inputs.no_epochs}} \\\n",
        "        --learning-rate ${{inputs.learning_rate}} \\\n",
        "        --warmup-steps ${{inputs.warmup_steps}} \\\n",
        "        --weight-decay ${{inputs.weight_decay}} \\\n",
        "        --adam-beta1 ${{inputs.adam_beta1}} \\\n",
        "        --adam-beta2 ${{inputs.adam_beta2}} \\\n",
        "        --adam-epsilon ${{inputs.adam_epsilon}}\n",
        "    \"\"\",\n",
        "    display_name=\"HyperDrive_Step\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.sweep import Choice, BanditPolicy\n",
        "\n",
        "# we will reuse the command_job created before. we call it as a function so that we can apply inputs\n",
        "job_train_for_sweep = job_train(\n",
        "    # large checkpoints needs larger GPU VMs such as A100\n",
        "    base_checkpoint=Choice([\"bert-base-cased\", \"bert-large-cased\", 'microsoft/deberta-v3-base', \"microsoft/deberta-v3-small\", \"distilbert-base-uncased\", \"bert-base-uncased\"]),\n",
        "    learning_rate=Choice([5.5e-5, 5e-5, 4.5e-5, 4e-5, 5.5e-5, 6e-5, 2.0e-5, 2.5e-5, 3.0e-5, 3.5e-5, 6.5e-5]),\n",
        "    batch_size=Choice([8]),\n",
        "    no_epochs=Choice([4]),\n",
        "    warmup_steps=Choice([0]),\n",
        "    weight_decay=Choice([0.0]),\n",
        "    adam_beta1=Choice([0.9]),\n",
        "    adam_beta2=Choice([0.999]),\n",
        "    adam_epsilon=Choice([1e-8])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting environments/conda_dependencies_cpu_v2.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile environments/conda_dependencies_cpu_v2.yml\n",
        "\n",
        "channels:\n",
        "  - pytorch\n",
        "  - anaconda\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.7\n",
        "  - pip\n",
        "  - pip:\n",
        "      - azure-ai-ml\n",
        "      - mlflow\n",
        "      - azureml-mlflow\n",
        "      - nvitop\n",
        "      - transformers\n",
        "      - joblib\n",
        "      - datasets\n",
        "  - numpy\n",
        "  - pandas\n",
        "  - shap\n",
        "  - scikit-learn\n",
        "name: sdk_v2_cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "env_name_non_gpu = 'sdk-v2-cpu'\n",
        "\n",
        "try:\n",
        "    env_list = list(ml_client.environments.list(name=env_name_non_gpu))\n",
        "    env_v2 = env_list[0]\n",
        "except:\n",
        "    env_v2 = Environment(\n",
        "        image=\"mcr.microsoft.com/azureml/curated/sklearn-0.24-ubuntu18.04-py37-cpu:latest\",\n",
        "        conda_file='environments/conda_dependencies_cpu_v2.yml',\n",
        "        name=env_name_non_gpu,\n",
        "        description='This environment is curated to run sdk v2 for cpu base use-cases'\n",
        "    )\n",
        "\n",
        "    ml_client.environments.create_or_update(env_v2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name='sentiment_classifier_sdk_v2'\n",
        "\n",
        "job_register = command(\n",
        "    inputs=dict(\n",
        "        model_path=Input(type=\"custom_model\"),\n",
        "        model_name=model_name,\n",
        "        subscription_id=subscription_id,\n",
        "        resource_group=resource_group,\n",
        "        workspace=workspace\n",
        "    ),\n",
        "    outputs=dict(\n",
        "        linkage_data=Output(type=\"custom_model\")\n",
        "    ),\n",
        "    compute=cpu_compute_target,\n",
        "    environment=env_v2,\n",
        "    code=source_directory, # location of source code\n",
        "    command=\"\"\"\n",
        "    python register_model.py \\\n",
        "        --model-path ${{inputs.model_path}} \\\n",
        "        --model-name ${{inputs.model_name}} \\\n",
        "        --subscription-id ${{inputs.subscription_id}} \\\n",
        "        --resource-group ${{inputs.resource_group}} \\\n",
        "        --workspace ${{inputs.workspace}} \\\n",
        "        --model-data ${{outputs.linkage_data}}\n",
        "    \"\"\",\n",
        "    display_name=\"Register_Best_Model\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_deploy = command(\n",
        "    inputs=dict(\n",
        "        endpoint_name='sentiment-endpoint-sdkv2',\n",
        "        linkage_data=Input(type=\"custom_model\"),\n",
        "        model_name=model_name,\n",
        "        environment_name=env_name,\n",
        "        subscription_id=subscription_id,\n",
        "        resource_group=resource_group,\n",
        "        workspace=workspace\n",
        "    ),\n",
        "    compute=cpu_compute_target,\n",
        "    environment=env_v2,\n",
        "    code=source_directory, # location of source code\n",
        "    command=\"\"\"\n",
        "    python deploy_model.py \\\n",
        "        --endpoint-name ${{inputs.endpoint_name}} \\\n",
        "        --model-name ${{inputs.model_name}} \\\n",
        "        --environment-name ${{inputs.environment_name}} \\\n",
        "        --subscription-id ${{inputs.subscription_id}} \\\n",
        "        --resource-group ${{inputs.resource_group}} \\\n",
        "        --workspace ${{inputs.workspace}} \\\n",
        "        --model-data ${{inputs.linkage_data}}\n",
        "    \"\"\",\n",
        "    display_name=\"Deploy_Latest_Model\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "@pipeline()\n",
        "def pipeline():\n",
        "    hyper_drive = job_train_for_sweep.sweep(\n",
        "        compute=compute_target,\n",
        "        sampling_algorithm=\"random\",\n",
        "        primary_metric=\"test_auc_weighted\",\n",
        "        goal=\"Maximize\",\n",
        "        max_total_trials=20,\n",
        "        max_concurrent_trials=4,\n",
        "        early_termination_policy=BanditPolicy(\n",
        "            slack_factor=0.1, evaluation_interval=5\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    reg_step = job_register(model_path=hyper_drive.outputs.model_output)\n",
        "    dep_step = job_deploy(linkage_data=reg_step.outputs.linkage_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_job = pipeline_construction()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: the provided asset name 'nlp-accelerator-sdk-v2' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'sdk-v2-cpu' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'sdk-v2-cpu' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'sdk-v2-cpu' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'sdk-v2-cpu' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'nlp-accelerator-sdk-v2' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'sdk-v2-cpu' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'sdk-v2-cpu' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'sdk-v2-cpu' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'sdk-v2-cpu' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'nlp-accelerator-sdk-v2' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'sdk-v2-cpu' will not be used for anonymous registration\n",
            "Warning: the provided asset name 'sdk-v2-cpu' will not be used for anonymous registration\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Bad pipe message: %s [b'\\xa9\\xadp\\xe6f\\xf7\\x03\\x80\\xc1\\xd42k\\x07\\x93\\x0b\\x05\\xdd3 \\x06k\\xf5<\\xa2\\xa2[\\x17\\xe7#\\xcaTX\\xf8eM\\x98\\xe6x\\x1d\\xf7Z\\xe3\\xd8\\x9eB\\x1bq\\xea\\xb22\\xcc\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08']\n",
            "Bad pipe message: %s [b'\\x01\\x05\\x01\\x06\\x01']\n",
            "Bad pipe message: %s [b'\\xdb8\\xa1\\x17\\x03\\x96\\xbcw\\xc1\\xd1\\xea\\xa3\\xe2\\xdd8\\x8e0\\xd7\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f']\n",
            "Bad pipe message: %s [b\"\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\", b'']\n",
            "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
            "Bad pipe message: %s [b\"E\\xd8\\xff\\x9a\\xed\\xc4\\xf7\\xeb\\xbd\\x87\\x12\\x87g\\x14\\x0c\\xdf\\xf9\\xbf\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\"]\n",
            "Bad pipe message: %s [b'']\n",
            "Bad pipe message: %s [b'', b'\\x02']\n",
            "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
            "Bad pipe message: %s [b'\\xd0\\n\\xb6\\xdb\\x9a\\x15_UD\\x9f\\t\\xa0\\xa3\\x9e\\xfc\\xf6\\x8a*\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00']\n",
            "Bad pipe message: %s [b'\\xc1\\xf3\\xd5?\\xc9\\xda{\\x81\\xae\\r\\xc3\\x94#(\\x88xC\\xdb\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t']\n",
            "Bad pipe message: %s [b'2\\xd5\\xbf\\xa5\\x9d\\xd2\\xb0`G\\xf1\\x0e8k\\x1bp\\x96\\xb4\\x06\\x00\\x00>\\xc0\\x14\\xc0', b'9\\x008\\x007\\x006\\xc0\\x0f']\n",
            "Bad pipe message: %s [b'U\\x10\\x80:\\x99UXo!!DL\\x02\\xb6t\\x87\\xd3/\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00']\n",
            "Bad pipe message: %s [b'\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00']\n",
            "Bad pipe message: %s [b\"\\x01J\\x9e\\xb1\\x88\\x98\\xa2/C\\xf7\\xaf\\xfe\\x08\\xbcd!\\xd8\\x1f\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\"]\n",
            "Bad pipe message: %s [b\"\\xe8zP\\x9e\\x95\\x00(HtS|\\xda\\x1e\\xd8\\xc5\\xdd\\xb5\\\\\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\", b'\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00']\n",
            "Bad pipe message: %s [b'\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06']\n"
          ]
        }
      ],
      "source": [
        "pipeline_job = ml_client.jobs.create_or_update(pipeline_job, experiment_name=experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "azureml_py310_sdkv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "2139c70ac98f3202d028164a545621647e07f47fd6f5d8ac55cf952bf7c15ed1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
