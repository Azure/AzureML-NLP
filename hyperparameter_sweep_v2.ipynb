{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End-to-End Text Classification Pipeline (Sentiment Analysis) with Azure ML v2 and Transformers \n",
        "\n",
        "### Utilizing HyperDrive for hyperparameter tuning and Managed Online Endpoint for Effective Model Training and Deployment\n",
        "\n",
        "This pipeline represents the end-to-end process of training, tuning, registering, and deploying the sentiment analysis model using AzureML  SDK v2. \n",
        "It starts by setting up the necessary AzureML workspace and compute resources, configuring the environment, and preparing the data by downloading it from a URL, parsing the JSON, and creating a Pandas DataFrame. The data is then split into train, validation, and test sets and registered as datasets in the AzureML workspace. The script then creates a pipeline consisting of a HyperDriveStep for hyperparameter tuning and several PythonScriptSteps for testing, registering, and deploying the best model as a web service.\n",
        "The dataset used is Amazon product reviews in the Automotive category. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Libraries and Setting Up Workspace\n",
        "In this section, we import the required libraries and set up the Azure Machine Learning workspace by providing the subscription ID, resource group, and workspace name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668533978662
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Enter details of your AML workspace\n",
        "subscription_id = \"<subscription_id>\"\n",
        "resource_group = \"<resource_group>\"\n",
        "workspace = \"<workspace>\"\n",
        "\n",
        "# get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
        ")\n",
        "\n",
        "print(ml_client.subscription_id, ml_client.resource_group_name, ml_client.workspace_name, sep='\\n')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing the Amazon review dataset\n",
        "The code in this section downloads the Amazon review dataset, processes it to add a sentiment column, and generates training, validation, and test sets. The resulting dataframes are registered as Azure ML datasets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to get larger datasets, visit: http://jmcauley.ucsd.edu/data/amazon/\n",
        "\n",
        "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Automotive_5.json.gz -P data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "\n",
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    yield eval(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "pdf_main = getDF('data/reviews_Automotive_5.json.gz')\n",
        "pdf_main.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf_main.loc[pdf_main['overall'] >= 4, 'sentiment'] = 1\n",
        "pdf_main.loc[pdf_main['overall'] < 3, 'sentiment'] = 0\n",
        "\n",
        "pdf_main.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def generate_datasets(pdf_target_training, label = 'sentiment'):\n",
        "    X_train, X_test_val, y_train, y_test_val = train_test_split(pdf_target_training.drop(label, axis=1), pdf_target_training[label],\n",
        "                                                        stratify=pdf_target_training[label],\n",
        "                                                        shuffle=True,\n",
        "                                                        test_size=0.20)\n",
        "\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val,\n",
        "                                                        stratify=y_test_val,\n",
        "                                                        shuffle=True,\n",
        "                                                        test_size=0.5)\n",
        "    pdf_X_train = X_train\n",
        "    pdf_X_val = X_val\n",
        "    pdf_X_test = X_test\n",
        "\n",
        "    pdf_X_train['sentiment'] = y_train\n",
        "    pdf_X_val['sentiment'] = y_val\n",
        "    pdf_X_test['sentiment'] = y_test\n",
        "    \n",
        "    print(f'Total records for: \"pdf_X_train\": [{pdf_X_train.shape[0]}]')\n",
        "    print(f'Total records for: \"pdf_X_val\": [{pdf_X_val.shape[0]}]')\n",
        "    print(f'Total records for: \"pdf_X_test\": [{pdf_X_test.shape[0]}]')\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    return pdf_X_train, pdf_X_val, pdf_X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf_train, pdf_val, pdf_test = generate_datasets(pdf_main[['reviewText', 'sentiment']].dropna(), 'sentiment')\n",
        "\n",
        "pdf_train.to_csv('data/pdf_train.csv')\n",
        "pdf_val.to_csv('data/pdf_val.csv')\n",
        "pdf_test.to_csv('data/pdf_test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "# === Note on path ===\n",
        "# can be can be a local path or a cloud path. AzureML supports https://`, `abfss://`, `wasbs://` and `azureml://` URIs.\n",
        "# Local paths are automatically uploaded to the default datastore in the cloud.\n",
        "# More details on supported paths: https://docs.microsoft.com/azure/machine-learning/how-to-read-write-data-v2#supported-paths\n",
        "\n",
        "def gen_input_data(url):\n",
        "    return Input(type=AssetTypes.URI_FILE, path=url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_train = gen_input_data('data/pdf_train.csv')\n",
        "ds_val = gen_input_data('data/pdf_val.csv')\n",
        "ds_test = gen_input_data('data/pdf_test.csv')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Environment \n",
        " This section sets up the environment for training the model by specifying the conda dependencies and creating an environment object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668533982929
        }
      },
      "outputs": [],
      "source": [
        "source_directory = \"./src_v2/\"\n",
        "experiment_name = 'transformer_hp_v2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668533983332
        }
      },
      "outputs": [],
      "source": [
        "%%writefile environments/conda_dependencies.yml\n",
        "\n",
        "channels:\n",
        "  - pytorch\n",
        "  - anaconda\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.7\n",
        "  - pip=21.1.2\n",
        "  - pip:\n",
        "      - azure-ai-ml==1.2.0\n",
        "      - mlflow== 1.26.1\n",
        "      - azureml-mlflow==1.42.0\n",
        "      - nvitop\n",
        "      - transformers\n",
        "      - inference-schema\n",
        "      - joblib\n",
        "      - datasets\n",
        "  - numpy~=1.21.6\n",
        "  - pandas~=1.1.5\n",
        "  - shap=0.39.0\n",
        "  - scikit-learn~=0.22.1\n",
        "  - pytorch==1.7.1\n",
        "name: nlp_training_environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "env_name = 'nlp-accelerator-sdk-v2'\n",
        "env_list = list(ml_client.environments.list(name=env_name))\n",
        "if len(env_list) > 0:\n",
        "    env = env_list[0]\n",
        "else:\n",
        "    env = Environment(\n",
        "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu20.04:latest\",\n",
        "        conda_file='environments/conda_dependencies.yml',\n",
        "        name=env_name,\n",
        "        description='This environment is curated to run NLP Transformer based models using AML SDK-v2 and native MLFlow integration'\n",
        "    )\n",
        "\n",
        "    ml_client.environments.create_or_update(env)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuring Compute Targets\n",
        "In this section, we configure the compute targets for training and deployment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668533987254
        }
      },
      "outputs": [],
      "source": [
        "cluster_name = \"a100-cluster\"\n",
        "compute_target = ml_client.compute.get(cluster_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cpu_compute_target = ml_client.compute.get(\"cpu-cluster\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the Training Job\n",
        "The training job is defined by specifying the inputs, outputs, compute target, environment, and the command to run the training script. This job trains the sentiment analysis model using the preprocessed data and the specified environment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "job_train = command(\n",
        "    inputs=dict(\n",
        "        training_dataset=ds_train,\n",
        "        val_dataset=ds_val,\n",
        "        test_dataset=ds_test,\n",
        "        target_name='sentiment', \n",
        "        text_field_name='reviewText',\n",
        "        is_test=1,\n",
        "        is_final=0,\n",
        "        is_local=0,\n",
        "        is_jump=0,\n",
        "        evaluation_strategy='epoch',\n",
        "        collect_resource_utilization=1,\n",
        "        resource_utilization_interval=5.0, # seconds\n",
        "        base_checkpoint='bert-base-cased',\n",
        "        batch_size=8,\n",
        "        no_epochs=4,\n",
        "        learning_rate=5.5e-5,\n",
        "        warmup_steps=0,\n",
        "        weight_decay=0.0,\n",
        "        adam_beta1=0.9,\n",
        "        adam_beta2=0.999,\n",
        "        adam_epsilon=1e-8\n",
        "    ),\n",
        "    outputs=dict(\n",
        "        model_output=Output(type=\"custom_model\")\n",
        "    ),\n",
        "    compute=compute_target,\n",
        "    environment=env,\n",
        "    code=source_directory, # location of source code\n",
        "    command=\"\"\"\n",
        "    python train_transformer.py \\\n",
        "        --collect-resource-utilization ${{inputs.collect_resource_utilization}} \\\n",
        "        --resource-utilization-interval ${{inputs.resource_utilization_interval}} \\\n",
        "        --target-name ${{inputs.target_name}} \\\n",
        "        --training-dataset ${{inputs.training_dataset}} \\\n",
        "        --val-dataset ${{inputs.val_dataset}} \\\n",
        "        --test-dataset ${{inputs.test_dataset}} \\\n",
        "        --model-path ${{outputs.model_output}} \\\n",
        "        --text-field-name ${{inputs.text_field_name}} \\\n",
        "        --is-test ${{inputs.is_test}} \\\n",
        "        --is-final ${{inputs.is_final}} \\\n",
        "        --is-local ${{inputs.is_local}} \\\n",
        "        --is-jump ${{inputs.is_jump}} \\\n",
        "        --evaluation-strategy ${{inputs.evaluation_strategy}} \\\n",
        "        --base-checkpoint ${{inputs.base_checkpoint}} \\\n",
        "        --batch-size ${{inputs.batch_size}} \\\n",
        "        --no-epochs ${{inputs.no_epochs}} \\\n",
        "        --learning-rate ${{inputs.learning_rate}} \\\n",
        "        --warmup-steps ${{inputs.warmup_steps}} \\\n",
        "        --weight-decay ${{inputs.weight_decay}} \\\n",
        "        --adam-beta1 ${{inputs.adam_beta1}} \\\n",
        "        --adam-beta2 ${{inputs.adam_beta2}} \\\n",
        "        --adam-epsilon ${{inputs.adam_epsilon}}\n",
        "    \"\"\",\n",
        "    display_name=\"HyperDrive_Step\",\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning Configuration\n",
        "In the hyperparameter tuning section, we define the hyperparameter search space and configure the hyperparameter tuning job using the BanditPolicy for early termination. This allows us to find the best model by searching through different combinations of hyperparameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.sweep import Choice, BanditPolicy\n",
        "\n",
        "# we will reuse the command_job created before. we call it as a function so that we can apply inputs\n",
        "job_train_for_sweep = job_train(\n",
        "    # large checkpoints needs larger GPU VMs such as A100\n",
        "    base_checkpoint=Choice([\"bert-base-cased\"]), #, \"bert-base-cased\"]), # , \"bert-large-cased\", \"microsoft/deberta-v3-small\", \"distilbert-base-uncased\", \"bert-base-uncased\"]),\n",
        "    batch_size=Choice([8]),\n",
        "    no_epochs=Choice([4]),\n",
        "    learning_rate=Choice([5.5e-5, 5e-5, 4.5e-5, 4e-5, 5.5e-5, 6e-5, 3.5e-5, 6.5e-5]),\n",
        "    warmup_steps=Choice([0]),\n",
        "    weight_decay=Choice([0.0]),\n",
        "    adam_beta1=Choice([0.9]),\n",
        "    adam_beta2=Choice([0.999]),\n",
        "    adam_epsilon=Choice([1e-8])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile environments/conda_dependencies_cpu_v2.yml\n",
        "\n",
        "channels:\n",
        "  - pytorch\n",
        "  - anaconda\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.7\n",
        "  - pip\n",
        "  - pip:\n",
        "      - azure-ai-ml\n",
        "      - mlflow\n",
        "      - azureml-mlflow\n",
        "      - nvitop\n",
        "      - transformers\n",
        "      - joblib\n",
        "      - datasets\n",
        "  - numpy\n",
        "  - pandas\n",
        "  - shap\n",
        "  - scikit-learn\n",
        "name: sdk_v2_cpu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up the Training Environment\n",
        "To set up the environment for training the model, we specify the conda dependencies and create an environment object. This object contains all the necessary packages and configurations needed for running the training script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "env_name_non_gpu = 'sdk-v2-cpu'\n",
        "\n",
        "try:\n",
        "    env_list = list(ml_client.environments.list(name=env_name_non_gpu))\n",
        "    env_v2 = env_list[0]\n",
        "except:\n",
        "    env_v2 = Environment(\n",
        "        image=\"mcr.microsoft.com/azureml/curated/sklearn-0.24-ubuntu18.04-py37-cpu:latest\",\n",
        "        conda_file='environments/conda_dependencies_cpu_v2.yml',\n",
        "        name=env_name_non_gpu,\n",
        "        description='This environment is curated to run sdk v2 for cpu base use-cases'\n",
        "    )\n",
        "\n",
        "    ml_client.environments.create_or_update(env_v2)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Registration Job Definition\n",
        "The model registration job is defined to register the best model from the hyperparameter tuning step. This step makes the trained model available for deployment and further use in the Azure Machine Learning workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name='sentiment_classifier_sdk_v2'\n",
        "\n",
        "job_register = command(\n",
        "    inputs=dict(\n",
        "        model_path=Input(type=\"custom_model\"),\n",
        "        model_name=model_name,\n",
        "        subscription_id=subscription_id,\n",
        "        resource_group=resource_group,\n",
        "        workspace=workspace\n",
        "    ),\n",
        "    outputs=dict(\n",
        "        linkage_data=Output(type=\"custom_model\")\n",
        "    ),\n",
        "    compute=cpu_compute_target,\n",
        "    environment=env_v2,\n",
        "    code=source_directory, # location of source code\n",
        "    command=\"\"\"\n",
        "    python register_model.py \\\n",
        "        --model-path ${{inputs.model_path}} \\\n",
        "        --model-name ${{inputs.model_name}} \\\n",
        "        --subscription-id ${{inputs.subscription_id}} \\\n",
        "        --resource-group ${{inputs.resource_group}} \\\n",
        "        --workspace ${{inputs.workspace}} \\\n",
        "        --model-data ${{outputs.linkage_data}}\n",
        "    \"\"\",\n",
        "    display_name=\"Register_Best_Model\",\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Deployment Job Definition\n",
        "In the model deployment section, we define the model deployment job, which deploys the registered model as a web service. This allows users to access and use the sentiment analysis model through a REST API. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_deploy = command(\n",
        "    inputs=dict(\n",
        "        endpoint_name='sentiment-endpoint-sdkv2',\n",
        "        linkage_data=Input(type=\"custom_model\"),\n",
        "        model_name=model_name,\n",
        "        environment_name=env_name,\n",
        "        subscription_id=subscription_id,\n",
        "        resource_group=resource_group,\n",
        "        workspace=workspace\n",
        "    ),\n",
        "    compute=cpu_compute_target,\n",
        "    environment=env_v2,\n",
        "    code=source_directory, # location of source code\n",
        "    command=\"\"\"\n",
        "    python deploy_model.py \\\n",
        "        --endpoint-name ${{inputs.endpoint_name}} \\\n",
        "        --model-name ${{inputs.model_name}} \\\n",
        "        --environment-name ${{inputs.environment_name}} \\\n",
        "        --subscription-id ${{inputs.subscription_id}} \\\n",
        "        --resource-group ${{inputs.resource_group}} \\\n",
        "        --workspace ${{inputs.workspace}} \\\n",
        "        --model-data ${{inputs.linkage_data}}\n",
        "    \"\"\",\n",
        "    display_name=\"Deploy_Latest_Model\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating and Chaining the Pipeline with the list of job steps\n",
        "create the pipeline by chaining the hyperparameter tuning, model registration, and deployment jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "@pipeline()\n",
        "def pipeline_construction():\n",
        "    \"\"\"The hello world pipeline job.\"\"\"\n",
        "    hyper_drive = job_train_for_sweep.sweep(\n",
        "        compute=compute_target,\n",
        "        sampling_algorithm=\"random\",\n",
        "        primary_metric=\"test_f1_weighted\",\n",
        "        goal=\"Maximize\",\n",
        "        max_total_trials=1,\n",
        "        max_concurrent_trials=1,\n",
        "        early_termination_policy=BanditPolicy(\n",
        "            slack_factor=0.1, evaluation_interval=5\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    reg_step = job_register(model_path=hyper_drive.outputs.model_output)\n",
        "    dep_step = job_deploy(linkage_data=reg_step.outputs.linkage_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_job = pipeline_construction()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_job = ml_client.jobs.create_or_update(pipeline_job, experiment_name=experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "azureml_py310_sdkv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "2139c70ac98f3202d028164a545621647e07f47fd6f5d8ac55cf952bf7c15ed1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
