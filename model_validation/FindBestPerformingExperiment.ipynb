{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure ML SDK Version:  1.44.0\n",
            "MLflow version: 1.26.1\n"
          ]
        }
      ],
      "source": [
        "import azureml\n",
        "import mlflow\n",
        "from azureml.core import Workspace, Dataset, Environment\n",
        "\n",
        "# check core SDK version number\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
        "print(\"MLflow version:\", mlflow.version.VERSION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Workspace name: scsccps-dsai-aide-dev-mlw\n",
            "Azure region: canadacentral\n",
            "Subscription id: 105efa68-0ff4-486f-ae3a-86e28a447237\n",
            "Resource group: scsc-dsai-aide-dev-rg\n"
          ]
        }
      ],
      "source": [
        "ws = Workspace.from_config()\n",
        "# mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep='\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "# script_folder = './project'\n",
        "# os.makedirs(script_folder, exist_ok=True)\n",
        "\n",
        "exp = Experiment(workspace=ws, name='transformer_hp')\n",
        "# mlflow.set_experiment('transformer_hp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "counter = 0\n",
        "best_temporal_f1_weighted = 0.0\n",
        "all_runs = exp.get_runs(include_children=True)\n",
        "dic_runs = {}\n",
        "test = False\n",
        "run_id = 'HD_03ef2102-1cf6-49a7-84a7-140720e57834_2'\n",
        "\n",
        "for i, run in enumerate(all_runs):\n",
        "    if test:\n",
        "        if run_id in run.id:\n",
        "            metrics = run.get_metrics()\n",
        "            dic_runs[run.id] = {\n",
        "                'run': run,\n",
        "                'metrics': metrics\n",
        "            }\n",
        "            break\n",
        "        else:\n",
        "            continue\n",
        "    else:\n",
        "        metrics = run.get_metrics()\n",
        "        if 'temporal_test_f1_weighted' in metrics:\n",
        "            dic_runs[run.id] = {\n",
        "                'run': run,\n",
        "                'metrics': metrics\n",
        "            }\n",
        "        counter+=1\n",
        "    \n",
        "counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6946529747604724"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temporal_test_f1_weighted = dic_runs['HD_03ef2102-1cf6-49a7-84a7-140720e57834_2']['metrics'][metric_name]\n",
        "\n",
        "if (type(temporal_test_f1_weighted) == list):\n",
        "        temporal_test_f1_weighted = float(temporal_test_f1_weighted[0])\n",
        "\n",
        "temporal_test_f1_weighted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dic_runs['HD_bc6f1629-7b96-45fc-9858-b4c053fdfbe0_5']['run'].get_details()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run id: HD_03ef2102-1cf6-49a7-84a7-140720e57834_1\n",
            "Temporal test date: 202207\n",
            "temporal_test_f1_weighted: 0.7093031060452879 - temporal_test_f1: 0.552862822070211\n",
            "Train dataset name: owner_g_classfication_train, V:55\n",
            "Train dataset name: owner_g_classfication_temporal_test, V:50\n"
          ]
        }
      ],
      "source": [
        "\n",
        "metric_name = \"temporal_test_f1_weighted\" # \"temporal_test_f1_weighted\"\n",
        "second_metric = \"temporal_test_f1\"\n",
        "temporal_test_date = '202207'\n",
        "li_test_values = []\n",
        "best_performing_run = None\n",
        "\n",
        "for run_id in dic_runs:\n",
        "    if 'HD_e8d5cb0f-6d51-4837-bef6-be3e8072eb38' in run_id: # or 'HD_432106ac-4483-44ef-94bc-ba7399357160' in run_id or 'transformer_hp_1659502191570' in run_id:\n",
        "        continue\n",
        "\n",
        "    temporal_test_f1_weighted = dic_runs[run_id]['metrics'][metric_name]\n",
        "    if (type(temporal_test_f1_weighted) == list):\n",
        "        temporal_test_f1_weighted = float(temporal_test_f1_weighted[0])\n",
        "    else:\n",
        "        temporal_test_f1_weighted = float(temporal_test_f1_weighted)\n",
        "\n",
        "    if len(li_test_values) > 0 and temporal_test_f1_weighted > max(li_test_values):\n",
        "        if temporal_test_date == None:\n",
        "            best_performing_run = dic_runs[run_id]\n",
        "\n",
        "        if temporal_test_date:\n",
        "            dataset = dic_runs[run_id]['run'].get_details()['inputDatasets'][0]\n",
        "            if 'temporal_test_date' in dataset['dataset'].tags:\n",
        "                if dataset['dataset'].tags['temporal_test_date'] == temporal_test_date:\n",
        "                    best_performing_run = dic_runs[run_id]\n",
        "\n",
        "    if temporal_test_date == None:\n",
        "        li_test_values.append(temporal_test_f1_weighted)\n",
        "\n",
        "    if temporal_test_date:\n",
        "        dataset = dic_runs[run_id]['run'].get_details()['inputDatasets'][0]\n",
        "        if 'temporal_test_date' in dataset['dataset'].tags:\n",
        "            if dataset['dataset'].tags['temporal_test_date'] == temporal_test_date:\n",
        "                # best_performing_run = dic_runs[run_id]\n",
        "                li_test_values.append(temporal_test_f1_weighted)\n",
        "\n",
        "if not best_performing_run:\n",
        "    print('No run is found')\n",
        "else:\n",
        "    run = best_performing_run['run']\n",
        "\n",
        "    train_dataset = None\n",
        "    temporal_dataset = None\n",
        "\n",
        "    for dataset in run.get_details()['inputDatasets']:\n",
        "        if dataset['dataset'].name == 'owner_g_classfication_train':\n",
        "            train_dataset = dataset\n",
        "        elif dataset['dataset'].name == 'owner_g_classfication_temporal_test':\n",
        "            temporal_dataset = dataset\n",
        "\n",
        "    print(f'run id: {run.id}')\n",
        "    print(f'Temporal test date: {temporal_test_date}')\n",
        "    print(f'{metric_name}: {best_performing_run[\"metrics\"][metric_name]} - {second_metric}: {best_performing_run[\"metrics\"][second_metric]}')\n",
        "    print(f'Train dataset name: {train_dataset[\"dataset\"].name}, V:{train_dataset[\"dataset\"].version}')\n",
        "    print(f'Train dataset name: {temporal_dataset[\"dataset\"].name}, V:{temporal_dataset[\"dataset\"].version}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_train = Dataset.get_by_name(ws, name=\"owner_g_classfication_train\", version=train_dataset[\"dataset\"].version)\n",
        "ds_val = Dataset.get_by_name(ws, name=\"owner_g_classfication_val\", version=train_dataset[\"dataset\"].version)\n",
        "ds_test = Dataset.get_by_name(ws, name=\"owner_g_classfication_test\", version=train_dataset[\"dataset\"].version)\n",
        "ds_temporal_test = Dataset.get_by_name(ws, name=\"owner_g_classfication_temporal_test\", version=temporal_dataset[\"dataset\"].version)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>transformer_hp</td><td>HD_03ef2102-1cf6-49a7-84a7-140720e57834_1</td><td>azureml.scriptrun</td><td>Completed</td><td><a href=\"https://ml.azure.com/runs/HD_03ef2102-1cf6-49a7-84a7-140720e57834_1?wsid=/subscriptions/105efa68-0ff4-486f-ae3a-86e28a447237/resourcegroups/scsc-dsai-aide-dev-rg/workspaces/scsccps-dsai-aide-dev-mlw&amp;tid=8c1a4d93-d828-4d0e-9303-fd3bd611c822\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
            ],
            "text/plain": [
              "Run(Experiment: transformer_hp,\n",
              "Id: HD_03ef2102-1cf6-49a7-84a7-140720e57834_1,\n",
              "Type: azureml.scriptrun,\n",
              "Status: Completed)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azureml.train.hyperdrive import HyperDriveRun\n",
        "\n",
        "run_hp = best_performing_run['run']\n",
        "run_hp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir = 'output'\n",
        "prefix_path = \"outputs/model\"\n",
        "model_directory = f'{dir}/{prefix_path}'\n",
        "\n",
        "isdir = os.path.isdir(dir)\n",
        "if isdir:\n",
        "    shutil.rmtree(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_hp.download_files(prefix=prefix_path, output_directory=dir, timeout_seconds=6000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf_train = ds_train.to_pandas_dataframe()\n",
        "li_target = list(pdf_train['target'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(f\"{model_directory}/target_list.json\", \"wb\") as outfile:\n",
        "    pickle.dump(li_target, outfile)\n",
        "    # outfile.write(\"\\n\".join(li_target))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f\"{model_directory}/target_list.json\", \"rb\") as outfile:\n",
        "    li_target = pickle.load(outfile)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "121"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(li_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import glob\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn import preprocessing\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers.integrations import AzureMLCallback\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from datasets import Dataset, DatasetDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_directory = 'outputs/model'\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_directory, num_labels=51)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "le=joblib.load(model_directory + '/labelEncoder.joblib')\n",
        "le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "model.zero_grad()\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8099, 2)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf_temporal_test = ds_temporal_test.to_pandas_dataframe()\n",
        "pdf_temporal_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "sys.path.append(os.path.join(os.path.join(os.getcwd(), \"..\"), 'project'))\n",
        "from train_transformer import get_model, adjust_tokenizer, compute_metrics, get_encode_labels, tokenize_function, generate_tokenized_dataset, get_datasets, test_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "fields = ['TEXT_FINAL', 'target', 'labels']\n",
        "target_name = 'target'\n",
        "text_field_name = 'TEXT_FINAL'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b454d861d5464f2db4b25958bbb86090",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "temporal_test_ds, tokenized_temporal_test_ds = generate_tokenized_dataset(pdf_temporal_test, fields, le, target_name, text_field_name, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(model=model, compute_metrics=compute_metrics, tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='1013' max='1013' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1013/1013 00:25]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "temporal_test_loss 1.03770112991333\n",
            "temporal_test_accuracy 0.6673252654976537\n",
            "temporal_test_precision 0.6867294283374241\n",
            "temporal_test_recall 0.5928634688970607\n",
            "temporal_test_f1 0.6211564949726264\n",
            "temporal_test_recall_weighted 0.6673252654976537\n",
            "temporal_test_precision_weighted 0.6965405885330128\n",
            "temporal_test_f1_weighted 0.665144170447547\n",
            "temporal_test_runtime 25.7525\n",
            "temporal_test_samples_per_second 314.455\n",
            "temporal_test_init_mem_cpu_alloc_delta 8192\n",
            "temporal_test_init_mem_gpu_alloc_delta 0\n",
            "temporal_test_init_mem_cpu_peaked_delta 0\n",
            "temporal_test_init_mem_gpu_peaked_delta 0\n",
            "temporal_test_mem_cpu_alloc_delta 16289792\n",
            "temporal_test_mem_gpu_alloc_delta 0\n",
            "temporal_test_mem_cpu_peaked_delta 0\n",
            "temporal_test_mem_gpu_peaked_delta 291664896\n"
          ]
        }
      ],
      "source": [
        "prefix = 'temporal_test'\n",
        "\n",
        "test_result = trainer.predict(tokenized_temporal_test_ds)\n",
        "\n",
        "metrics = test_result.metrics.keys()\n",
        "# print(f'len(metrics): {metrics}')\n",
        "\n",
        "for m in metrics:\n",
        "    print(f'{prefix}_{m.replace(\"test_\", \"\")}', f'{test_result.metrics[m]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = np.argmax(test_result.predictions, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = test_result.label_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
          ]
        }
      ],
      "source": [
        "accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "recall = recall_score(y_true=labels, y_pred=pred, average='macro')\n",
        "precision = precision_score(y_true=labels, y_pred=pred, average='macro')\n",
        "f1 = f1_score(y_true=labels, y_pred=pred, average='macro')\n",
        "\n",
        "recall_weighted = recall_score(y_true=labels, y_pred=pred, average='weighted')\n",
        "precision_weighted = precision_score(y_true=labels, y_pred=pred, average='weighted')\n",
        "f1_weighted = f1_score(y_true=labels, y_pred=pred, average='weighted')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.665144170447547"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1_weighted\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
